{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO6EglxXRkzV798xon405J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zuy3UmATHz7g"
      },
      "outputs": [],
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "KhbpDrBWK3tM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function calculates the straight-line distance between two points\n",
        "def euclidean_distance(x1, x2):\n",
        "    # 1. Subtraction: Find the difference between coordinates\n",
        "    # 2. Square: Ensure values are positive and penalize larger differences\n",
        "    # 3. Sum: Add all the squared differences together\n",
        "    # 4. Sqrt: Take the square root to get the final distance\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        # Store the number of neighbors (k) we want to look at.\n",
        "        # If k=3, we look at the 3 closest points to make a decision.\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # In KNN, \"training\" is just storing the data.\n",
        "        # X_train are the features (e.g., height/weight)\n",
        "        # y_train are the labels (e.g., 'Athlete' or 'Non-Athlete')\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        # This takes a list of multiple new points and predicts labels for each\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # 1. Compute the distance between the new point 'x'\n",
        "        # and every single point in our training set\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # 2. Sort the distances and get the indices of the 'k' smallest ones\n",
        "        # np.argsort returns the positions (indices) of the values in sorted order\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "        # 3. Use those indices to find the actual labels (classes) of those neighbors\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # 4. Find the most common label among the neighbors (Majority Vote)\n",
        "        # Counter(...).most_common(1) returns a list like [('label', count)]\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "\n",
        "        # Return the label itself (the first element of the first tuple)\n",
        "        return most_common[0][0]"
      ],
      "metadata": {
        "id": "7Po_UJnoIF5F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "eaBFuMwjNmT9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, lr=0.001, n_iters=1000):\n",
        "        # lr (Learning Rate): How big of a step we take during optimization\n",
        "        self.lr = lr\n",
        "        # n_iters: How many times we loop through the data to \"learn\"\n",
        "        self.n_iters = n_iters\n",
        "        # We initialize weights and bias as None because we don't know\n",
        "        # the data shape (number of features) until we call fit()\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # X.shape gives (number of rows, number of columns)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights as a vector of zeros (one for each feature/column)\n",
        "        self.weights = np.zeros(n_features)\n",
        "        # Initialize bias (the intercept) as a single zero\n",
        "        self.bias = 0\n",
        "\n",
        "        # Optimization loop (Gradient Descent)\n",
        "        for _ in range(self.n_iters):\n",
        "            # 1. Linear Formula: y = (X * weights) + bias\n",
        "            # np.dot does the matrix multiplication between input and weights\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # 2. Calculate Gradients (the direction of steepest ascent)\n",
        "            # dw = (1/n) * sum of (X_transposed * (predictions - actual_y))\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            # db = (1/n) * sum of (predictions - actual_y)\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # 3. Update Parameters: Move weights in the opposite direction\n",
        "            # of the gradient to minimize the error (Learning Rate * Gradient)\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Once the model is trained, use the final weights and bias\n",
        "        # to calculate the output for new data\n",
        "        y_pred = np.dot(X, self.weights) + self.bias\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "wkVUGDEyNpHe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "EGsDXKDWUeXD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticReg:\n",
        "    # 1. Initialize the model's hyperparameters\n",
        "    def __init__(self, logreg=0.001, n_iter=1000):\n",
        "        self.logreg = logreg  # This is the learning rate (step size)\n",
        "        self.n_iter = n_iter  # Number of times to run gradient descent\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Get dimensions: X is a matrix of (samples, features)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights as zeros (one for each feature) and bias as 0\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient Descent: The core learning process\n",
        "        for _ in range(self.n_iter):\n",
        "            # Step A: Calculate the linear combination (z = wx + b)\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Step B: Apply the Sigmoid function to get a probability (0 to 1)\n",
        "            # This is the \"Logistic\" part of Logistic Regression\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Step C: Calculate the gradients (the error derivatives)\n",
        "            # We compare our prediction (0.8) to the real label (1.0)\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Step D: Update weights and bias to reduce the error\n",
        "            self.weights -= self.logreg * dw\n",
        "            self.bias -= self.logreg * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Step 1: Calculate the linear output for new data\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "        # Step 2: Convert those values into probabilities using sigmoid\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "        # Step 3: Classification threshold\n",
        "        # If probability > 0.5, predict Class 1. Otherwise, predict Class 0.\n",
        "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "        return np.array(y_predicted_cls)\n",
        "\n",
        "    # The Sigmoid Function: It \"squashes\" any value into the range [0, 1]\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n"
      ],
      "metadata": {
        "id": "1q0N8kCqUkop"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "3DOBj_ommPeN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Identify the unique labels (e.g., [0, 1] or ['cat', 'dog'])\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # Initialize tables to store the 'stats' for each class\n",
        "        # We need a mean and variance for every feature in every class\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        # Prior is the probability of a class appearing in the dataset\n",
        "        self._prior = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            # Create a subset of X containing only rows belonging to class 'c'\n",
        "            X_c = X[y == c]\n",
        "\n",
        "            # Calculate mean and variance for each feature of this class\n",
        "            self._mean[idx, :] = X_c.mean(axis=0)\n",
        "            self._var[idx, :] = X_c.var(axis=0)\n",
        "\n",
        "            # Prior = (number of samples in class c) / (total samples)\n",
        "            self._prior[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict the label for every row in the input X\n",
        "        y_predict = [self._predict(x) for x in X]\n",
        "        return np.array(y_predict)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # Calculate the probability for each class\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            # We use Log transformation here.\n",
        "            # Adding logs is mathematically the same as multiplying probabilities,\n",
        "            # but it prevents \"numerical underflow\" (numbers getting too small for the computer).\n",
        "            prior = np.log(self._prior[idx])\n",
        "\n",
        "            # Class conditional is the likelihood of the data given the class\n",
        "            class_conditional = np.sum(np.log(self._pdf(idx, x)))\n",
        "\n",
        "            # Posterior = log(Prior) + log(Likelihood)\n",
        "            posterior = prior + class_conditional\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        # Pick the class with the highest posterior probability\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "\n",
        "    def _pdf(self, class_idx, x):\n",
        "        # Probability Density Function (Gaussian/Normal Distribution)\n",
        "        # This calculates \"how likely\" a value 'x' is given the mean and variance\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "\n",
        "        # The Gaussian formula: (1 / sqrt(2 * pi * var)) * exp(- (x - mean)^2 / (2 * var))\n",
        "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator"
      ],
      "metadata": {
        "id": "1hulRa_ulkfv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "XyTnLZUzrkEy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function to find the distance between two points\n",
        "def euclidean_distance(x1, x2):\n",
        "    # Standard formula: square root of the sum of squared differences\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "class KMeans:\n",
        "    def __init__(self, k=5, max_iters=100):\n",
        "        self.k = k # Number of clusters we want to find\n",
        "        self.max_iters = max_iters # Safety limit to stop the loop\n",
        "        self.centroids = [] # The 'center' point of each cluster\n",
        "        self.clusters = [[] for _ in range(self.k)] # List of indices for each cluster\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.X = X\n",
        "        self.n_samples, self.n_features = X.shape\n",
        "\n",
        "        # 1. Initialization: Pick K random data points to be our starting centers\n",
        "        random_sample_idxs = np.random.choice(self.n_samples, self.k, replace=False)\n",
        "        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
        "\n",
        "        # 2. Optimization Loop\n",
        "        for _ in range(self.max_iters):\n",
        "            # A. Assign every point to the nearest centroid\n",
        "            self.clusters = self._create_clusters(self.centroids)\n",
        "\n",
        "            # B. Calculate new centroids (move the centers to the middle of their clusters)\n",
        "            centroids_old = self.centroids\n",
        "            self.centroids = self._get_centroids(self.clusters)\n",
        "\n",
        "            # C. Check for convergence: if the centers stopped moving, we are done!\n",
        "            if self._is_converged(centroids_old, self.centroids):\n",
        "                break\n",
        "\n",
        "        # 3. Final labels: return which cluster index (0 to K-1) each point belongs to\n",
        "        return self._get_cluster_labels(self.clusters)\n",
        "\n",
        "    def _get_cluster_labels(self, clusters):\n",
        "        # Create an empty array to fill with cluster IDs\n",
        "        labels = np.empty(self.n_samples)\n",
        "        for cluster_idx, cluster in enumerate(clusters):\n",
        "            for sample_idx in cluster:\n",
        "                labels[sample_idx] = cluster_idx\n",
        "        return labels\n",
        "\n",
        "    def _create_clusters(self, centroids):\n",
        "        # Temporary storage for the points assigned to each cluster\n",
        "        clusters = [[] for _ in range(self.k)]\n",
        "        for idx, sample in enumerate(self.X):\n",
        "            # Find which center is closest to this specific point\n",
        "            centroid_idx = self._closest_centroid(sample, centroids)\n",
        "            clusters[centroid_idx].append(idx)\n",
        "        return clusters\n",
        "\n",
        "    def _closest_centroid(self, sample, centroids):\n",
        "        # Calculate distance from the point to every centroid\n",
        "        distances = [euclidean_distance(sample, point) for point in centroids]\n",
        "        # Return the index of the smallest distance\n",
        "        return np.argmin(distances)\n",
        "\n",
        "    def _get_centroids(self, clusters):\n",
        "        # Initialize an array for new centers\n",
        "        centroids = np.zeros((self.k, self.n_features))\n",
        "        for cluster_idx, cluster in enumerate(clusters):\n",
        "            # Calculate the mean (average position) of all points in this cluster\n",
        "            cluster_mean = np.mean(self.X[cluster], axis=0)\n",
        "            centroids[cluster_idx] = cluster_mean\n",
        "        return centroids\n",
        "\n",
        "    def _is_converged(self, centroids_old, centroids):\n",
        "        # Check if any of the centroids moved at all\n",
        "        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.k)]\n",
        "        return sum(distances) == 0"
      ],
      "metadata": {
        "id": "0qN_qEUbvRm2"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}